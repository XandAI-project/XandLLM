services:

  # ── Main inference server ───────────────────────────────────────────────────
  xandllm:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    image: xandllm:latest
    container_name: xandllm
    ports:
      - "11435:11435"
    volumes:
      - model-cache:/root/.cache/xandllm
      - ../config:/app/config:ro
    environment:
      - RUST_LOG=${RUST_LOG:-info}
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN:-}
      - XANDLLM_SERVER_HOST=0.0.0.0
      - XANDLLM_SERVER_PORT=11435
    command: >
      sh -c "
        xandllm pull bartowski/Meta-Llama-3.1-8B-Instruct-GGUF
          --revision main
          --gguf-file Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf
        && xandllm serve
          --model bartowski/Meta-Llama-3.1-8B-Instruct-GGUF
          --host 0.0.0.0
          --port 11435
      "
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:11435/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s

  # ── Integration tests ───────────────────────────────────────────────────────
  # Runs once after the server is healthy; exits 0 on success, 1 on failure.
  # Run with:  docker compose up --exit-code-from xandllm-test
  xandllm-test:
    image: alpine/curl:latest
    container_name: xandllm-test
    depends_on:
      xandllm:
        condition: service_healthy
    volumes:
      - ./integration_test.sh:/integration_test.sh:ro
    environment:
      - API_URL=http://xandllm:11435
      - TEST_MODEL_ID=bartowski/Meta-Llama-3.1-8B-Instruct-GGUF
    command: ["/bin/sh", "/integration_test.sh"]
    restart: "no"

  # ── Frontend UI ─────────────────────────────────────────────────────────────
  frontend:
    build:
      context: ../frontend
      dockerfile: Dockerfile
    image: xandllm-frontend:latest
    container_name: xandllm-frontend
    ports:
      - "3000:80"
    depends_on:
      xandllm:
        condition: service_healthy
    restart: unless-stopped

volumes:
  model-cache:
    driver: local
